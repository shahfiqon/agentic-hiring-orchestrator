# LLM Provider Configuration (default: external llama.cpp server)
LLM_PROVIDER=llamacpp-server

# llama.cpp Server Configuration (default provider - OpenAI-compatible API)
# Make sure your llama.cpp server is running on this URL
LLAMACPP_BASE_URL=http://localhost:8080/v1
LLAMACPP_API_KEY=not-needed

# Common LLM Settings
TEMPERATURE=0.1
MAX_TOKENS=2048

# OpenAI Configuration (alternative provider - only if LLM_PROVIDER=openai)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL_NAME=gpt-4o-2024-08-06

# LangSmith (Optional - for tracing/debugging)
LANGCHAIN_TRACING_V2=false
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=agentic-hiring-orchestrator

# Application Configuration
ENABLE_WORKING_MEMORY=true
ENABLE_PRODUCT_AGENT=false
MAX_PANEL_AGENTS=4
RUBRIC_CATEGORIES_COUNT=5

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# Logging
LOG_LEVEL=INFO
