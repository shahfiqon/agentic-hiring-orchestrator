{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Workflow Testing Notebook\n",
    "\n",
    "This notebook provides interactive testing of the hiring workflow components.\n",
    "\n",
    "Use this to:\n",
    "- Test individual components (rubric generation, agent evaluation, synthesis)\n",
    "- Explore the two-pass evaluation pattern\n",
    "- Visualize decision packets and interview plans\n",
    "- Debug workflow issues with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "# Import models\n",
    "from src.models.rubric import Rubric\n",
    "from src.models.review import AgentReview\n",
    "from src.models.memory import WorkingMemory\n",
    "from src.models.packet import DecisionPacket\n",
    "from src.models.interview import InterviewPlan\n",
    "\n",
    "# Import nodes\n",
    "from src.nodes.orchestrator import orchestrator_node\n",
    "from src.nodes.hr_agent import hr_agent_node, _extract_working_memory, _evaluate_with_memory\n",
    "from src.nodes.tech_agent import tech_agent_node\n",
    "from src.nodes.compliance_agent import compliance_agent_node\n",
    "from src.nodes.synthesis import synthesis_node\n",
    "\n",
    "# Import workflow\n",
    "from src.graph import run_hiring_workflow\n",
    "\n",
    "# Import config\n",
    "from src.config import get_settings\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load job descriptions\n",
    "job_backend = Path(\"../examples/sample_data/job_backend_engineer.txt\").read_text()\n",
    "job_ai = Path(\"../examples/sample_data/job_senior_ai_engineer.txt\").read_text()\n",
    "\n",
    "# Load resumes\n",
    "resume_strong = Path(\"../examples/sample_data/resume_strong_candidate.txt\").read_text()\n",
    "resume_moderate = Path(\"../examples/sample_data/resume_moderate_candidate.txt\").read_text()\n",
    "\n",
    "print(f\"Loaded {len(job_backend)} chars from backend job description\")\n",
    "print(f\"Loaded {len(job_ai)} chars from AI engineer job description\")\n",
    "print(f\"Loaded {len(resume_strong)} chars from strong candidate resume\")\n",
    "print(f\"Loaded {len(resume_moderate)} chars from moderate candidate resume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Rubric Generation (Orchestrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test orchestrator with AI engineer job\n",
    "initial_state = {\n",
    "    \"job_description\": job_ai,\n",
    "    \"resume\": resume_strong,\n",
    "    \"company_context\": \"We're a Series B startup building AI-powered hiring tools.\"\n",
    "}\n",
    "\n",
    "orchestrator_result = orchestrator_node(initial_state)\n",
    "rubric = orchestrator_result[\"rubric\"]\n",
    "\n",
    "print(f\"\\n=== Generated Rubric ===\")\n",
    "print(f\"Categories: {len(rubric.categories)}\")\n",
    "print(f\"\\nCategory Breakdown:\")\n",
    "for cat in rubric.categories:\n",
    "    print(f\"  - {cat.name} (weight: {cat.weight:.2f}, must-have: {cat.is_must_have})\")\n",
    "    print(f\"    {cat.description[:100]}...\")\n",
    "    print(f\"    Scoring levels: {len(cat.scoring_criteria)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Working Memory Extraction (Two-Pass Pattern - Pass 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state with rubric\n",
    "state_with_rubric = {\n",
    "    \"job_description\": job_ai,\n",
    "    \"resume\": resume_strong,\n",
    "    \"rubric\": rubric,\n",
    "}\n",
    "\n",
    "# Extract working memory for HR agent\n",
    "hr_memory = _extract_working_memory(state_with_rubric)\n",
    "\n",
    "print(f\"\\n=== HR Agent Working Memory ===\")\n",
    "print(f\"Agent Role: {hr_memory.agent_role}\")\n",
    "print(f\"Observations: {len(hr_memory.observations)}\")\n",
    "print(f\"Cross-references: {len(hr_memory.cross_references)}\")\n",
    "print(f\"Ambiguities: {len(hr_memory.ambiguities)}\")\n",
    "\n",
    "print(f\"\\nSample Observations:\")\n",
    "for obs in hr_memory.observations[:3]:\n",
    "    print(f\"  [{obs.observation_type.upper()}] {obs.category_name}\")\n",
    "    print(f\"    {obs.observation}\")\n",
    "\n",
    "if hr_memory.ambiguities:\n",
    "    print(f\"\\nAmbiguities to explore in interview:\")\n",
    "    for amb in hr_memory.ambiguities:\n",
    "        print(f\"  - {amb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Agent Evaluation (Two-Pass Pattern - Pass 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using working memory\n",
    "hr_review = _evaluate_with_memory(state_with_rubric, hr_memory)\n",
    "\n",
    "print(f\"\\n=== HR Agent Review ===\")\n",
    "print(f\"Agent: {hr_review.agent_role}\")\n",
    "print(f\"Categories Scored: {len(hr_review.category_scores)}\")\n",
    "\n",
    "print(f\"\\nScores:\")\n",
    "for score in hr_review.category_scores:\n",
    "    print(f\"  {score.category_name}: {score.score}/5 (confidence: {score.confidence})\")\n",
    "    if score.evidence:\n",
    "        print(f\"    Evidence: {score.evidence[0].text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Full Panel Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all panel agents\n",
    "hr_result = hr_agent_node(state_with_rubric)\n",
    "tech_result = tech_agent_node(state_with_rubric)\n",
    "compliance_result = compliance_agent_node(state_with_rubric)\n",
    "\n",
    "# Combine results\n",
    "panel_state = {\n",
    "    **state_with_rubric,\n",
    "    \"panel_reviews\": [\n",
    "        hr_result[\"panel_reviews\"][0],\n",
    "        tech_result[\"panel_reviews\"][0],\n",
    "        compliance_result[\"panel_reviews\"][0],\n",
    "    ],\n",
    "    \"agent_working_memory\": {\n",
    "        **hr_result[\"agent_working_memory\"],\n",
    "        **tech_result[\"agent_working_memory\"],\n",
    "        **compliance_result[\"agent_working_memory\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\n=== Panel Evaluation Complete ===\")\n",
    "print(f\"Total Reviews: {len(panel_state['panel_reviews'])}\")\n",
    "print(f\"Agents: {', '.join(panel_state['agent_working_memory'].keys())}\")\n",
    "\n",
    "# Compare scores across agents for first category\n",
    "first_category = rubric.categories[0].name\n",
    "print(f\"\\nScore comparison for '{first_category}':\")\n",
    "for review in panel_state['panel_reviews']:\n",
    "    for score in review.category_scores:\n",
    "        if score.category_name == first_category:\n",
    "            print(f\"  {review.agent_role}: {score.score}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Synthesis & Decision Packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run synthesis\n",
    "synthesis_result = synthesis_node(panel_state)\n",
    "\n",
    "decision_packet = synthesis_result[\"decision_packet\"]\n",
    "interview_plan = synthesis_result[\"interview_plan\"]\n",
    "disagreements = synthesis_result[\"disagreements\"]\n",
    "\n",
    "print(\"\\n=== Decision Packet ===\")\n",
    "print(f\"Weighted Average Score: {decision_packet.weighted_average_score:.2f}/5.0\")\n",
    "print(f\"Recommendation: {decision_packet.recommendation}\")\n",
    "print(f\"Confidence: {decision_packet.confidence}\")\n",
    "\n",
    "if decision_packet.must_have_gaps:\n",
    "    print(f\"\\n‚ö†Ô∏è  Must-Have Gaps ({len(decision_packet.must_have_gaps)}):\")\n",
    "    for gap in decision_packet.must_have_gaps:\n",
    "        print(f\"  - {gap}\")\n",
    "\n",
    "if disagreements:\n",
    "    print(f\"\\n‚ö†Ô∏è  Disagreements ({len(disagreements)}):\")\n",
    "    for dis in disagreements:\n",
    "        print(f\"  - {dis.category_name} (delta: {dis.score_delta})\")\n",
    "        print(f\"    Reason: {dis.reason[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: Interview Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Interview Plan ===\")\n",
    "print(f\"Priority Areas: {len(interview_plan.priority_areas_to_probe)}\")\n",
    "print(f\"Total Questions: {interview_plan.total_questions()}\")\n",
    "\n",
    "print(f\"\\nPriority Areas to Probe:\")\n",
    "for area in interview_plan.priority_areas_to_probe:\n",
    "    print(f\"  - {area}\")\n",
    "\n",
    "print(f\"\\nSample Interview Questions:\")\n",
    "for i, question in enumerate(interview_plan.questions[:5], 1):\n",
    "    print(f\"\\n{i}. [{question.suggested_interviewer}] {question.question}\")\n",
    "    print(f\"   Why ask: {question.why_ask}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Workflow Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete workflow end-to-end\n",
    "print(\"Running full hiring workflow...\\n\")\n",
    "\n",
    "final_state = run_hiring_workflow(\n",
    "    job_description=job_ai,\n",
    "    resume=resume_strong,\n",
    "    company_context=\"Series B startup building AI-powered hiring tools\",\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"         HIRING WORKFLOW RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Summary\n",
    "packet = final_state[\"decision_packet\"]\n",
    "plan = final_state[\"interview_plan\"]\n",
    "\n",
    "print(f\"\\nüìä Overall Score: {packet.weighted_average_score:.2f}/5.0\")\n",
    "print(f\"üìù Recommendation: {packet.recommendation}\")\n",
    "print(f\"üéØ Confidence: {packet.confidence.upper()}\")\n",
    "print(f\"\\nüìã Panel Reviews: {len(final_state['panel_reviews'])}\")\n",
    "print(f\"üí≠ Working Memory: {len(final_state['agent_working_memory'])} agents\")\n",
    "print(f\"‚ö° Disagreements: {len(final_state['disagreements'])}\")\n",
    "print(f\"‚ùì Interview Questions: {plan.total_questions()}\")\n",
    "print(f\"üéØ Priority Areas: {len(plan.priority_areas_to_probe)}\")\n",
    "\n",
    "if packet.must_have_gaps:\n",
    "    print(f\"\\n‚ö†Ô∏è  Critical Gaps:\")\n",
    "    for gap in packet.must_have_gaps:\n",
    "        print(f\"   - {gap}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create score comparison DataFrame\n",
    "score_data = []\n",
    "for review in final_state[\"panel_reviews\"]:\n",
    "    for score in review.category_scores:\n",
    "        score_data.append({\n",
    "            \"Agent\": review.agent_role,\n",
    "            \"Category\": score.category_name,\n",
    "            \"Score\": score.score,\n",
    "            \"Confidence\": score.confidence,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(score_data)\n",
    "print(\"\\n=== Score Comparison Table ===\")\n",
    "print(df.pivot(index=\"Category\", columns=\"Agent\", values=\"Score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON\n",
    "output_dir = Path(\"../examples/results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export decision packet\n",
    "with open(output_dir / \"decision_packet.json\", \"w\") as f:\n",
    "    json.dump(packet.model_dump(mode=\"json\"), f, indent=2, default=str)\n",
    "\n",
    "# Export interview plan\n",
    "with open(output_dir / \"interview_plan.json\", \"w\") as f:\n",
    "    json.dump(plan.model_dump(mode=\"json\"), f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n‚úÖ Results exported to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test validation errors\n",
    "try:\n",
    "    # Try to run workflow without job description\n",
    "    run_hiring_workflow(\n",
    "        job_description=\"\",  # Invalid: empty\n",
    "        resume=resume_strong,\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"‚úÖ Caught expected validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Exploration\n",
    "\n",
    "Use the cells below to explore specific aspects of the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore working memory for specific agent\n",
    "agent_role = \"hr_agent\"  # Change to explore different agents\n",
    "memory = final_state[\"agent_working_memory\"][agent_role]\n",
    "\n",
    "print(f\"\\n=== {agent_role.upper()} Working Memory ===\")\n",
    "print(f\"\\nObservations ({len(memory.observations)}):\")\n",
    "for obs in memory.observations:\n",
    "    print(f\"  [{obs.observation_type}] {obs.category_name}: {obs.observation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore disagreements in detail\n",
    "for dis in final_state[\"disagreements\"]:\n",
    "    print(f\"\\n=== Disagreement: {dis.category_name} ===\")\n",
    "    print(f\"Score Delta: {dis.score_delta}\")\n",
    "    print(f\"\\nAgent Scores:\")\n",
    "    for agent, score in dis.agent_scores.items():\n",
    "        print(f\"  {agent}: {score}/5\")\n",
    "    print(f\"\\nReason: {dis.reason}\")\n",
    "    print(f\"\\nResolution: {dis.resolution_approach}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare moderate vs strong candidate\n",
    "print(\"\\nComparing candidate evaluations...\\n\")\n",
    "\n",
    "# You can run the workflow with different resumes and compare results\n",
    "# This cell is left for manual experimentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
